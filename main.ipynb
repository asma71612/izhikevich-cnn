{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1. Setup and Imports\n",
    "Create a cell to import all necessary libraries and set up the device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2. Define the Izhikevich Neuron Module\n",
    "This cell defines the Izhikevich neuron dynamics. We use a simple discrete time approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Izhikevich Neuron Module\n",
    "class IzhikevichNeuron(nn.Module):\n",
    "    def __init__(self, a=0.02, b=0.2, c=-65, d=8, v_peak=30):\n",
    "        super(IzhikevichNeuron, self).__init__()\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "        self.c = c\n",
    "        self.d = d\n",
    "        self.v_peak = v_peak\n",
    "\n",
    "    def forward(self, v, u, I, dt=1.0):\n",
    "        # Update membrane potential and recovery variable\n",
    "        dv = (0.04 * v * v + 5 * v + 140 - u + I) * dt\n",
    "        v = v + dv\n",
    "        du = self.a * (self.b * v - u) * dt\n",
    "        u = u + du\n",
    "\n",
    "        # Generate spike (binary output)\n",
    "        spikes = (v >= self.v_peak).float()\n",
    "        # Reset dynamics after spike\n",
    "        v = spikes * self.c + (1 - spikes) * v\n",
    "        u = u + spikes * self.d\n",
    "        return v, u, spikes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3. Build a Spiking Convolutional Layer\n",
    "This cell creates a convolutional layer that applies a convolution then passes the result through the Izhikevich neuron dynamics over a series of time steps. In a real implementation, you might have a spike-wave tensor (time, batch, channels, height, width)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spike output shape: torch.Size([10, 1, 10, 24, 24])\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Define a Spiking Convolutional Layer Using Izhikevich Neurons\n",
    "class SpikingConvLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, neuron_params, padding=0):\n",
    "        super(SpikingConvLayer, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, padding=padding, bias=False)\n",
    "        self.neuron = IzhikevichNeuron(*neuron_params)\n",
    "    \n",
    "    def forward(self, input_spike_wave, time_steps, dt=1.0):\n",
    "        # Assume input_spike_wave shape: [batch, channels, height, width]\n",
    "        # We will simulate over \"time_steps\"\n",
    "        batch_size, _, H, W = input_spike_wave.shape\n",
    "        \n",
    "        # Initialize membrane potential and recovery variable (set to zeros)\n",
    "        # For simplicity, assume spatial dimensions reduce as per convolution parameters.\n",
    "        conv_out = self.conv(input_spike_wave)\n",
    "        v = torch.zeros_like(conv_out).to(input_spike_wave.device)\n",
    "        u = torch.zeros_like(conv_out).to(input_spike_wave.device)\n",
    "        \n",
    "        # Record spikes over time\n",
    "        spike_record = []\n",
    "        for t in range(time_steps):\n",
    "            # For demonstration, use the same convolution output as input current each time step.\n",
    "            # In a more complex model, you may have time-varying input.\n",
    "            I = conv_out  \n",
    "            v, u, spikes = self.neuron(v, u, I, dt)\n",
    "            spike_record.append(spikes)\n",
    "        \n",
    "        # Stack spike outputs: shape [time_steps, batch, out_channels, H_out, W_out]\n",
    "        spike_record = torch.stack(spike_record, dim=0)\n",
    "        return spike_record, v, u\n",
    "\n",
    "# Test the layer with a dummy input\n",
    "dummy_input = torch.rand(1, 1, 28, 28).to(device)\n",
    "layer = SpikingConvLayer(in_channels=1, out_channels=10, kernel_size=5, neuron_params=(0.02, 0.2, -65, 8))\n",
    "spike_output, v_final, u_final = layer(dummy_input, time_steps=10)\n",
    "print(\"Spike output shape:\", spike_output.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4. Data Transformation: Image to Spike-Wave Conversion\n",
    "Here we define a simple transformation that converts an image into a “spike-wave” tensor. In this example, we use a latency coding approach where the intensity is mapped to a spike time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9.91M/9.91M [00:03<00:00, 2.59MB/s]\n",
      "100%|██████████| 28.9k/28.9k [00:00<00:00, 1.10MB/s]\n",
      "100%|██████████| 1.65M/1.65M [00:00<00:00, 9.47MB/s]\n",
      "100%|██████████| 4.54k/4.54k [00:00<00:00, 1.64MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spike-wave shape: torch.Size([10, 1, 28, 28])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 4: Data Transformation - Convert Images to Spike-Wave Tensors\n",
    "def image_to_spike_wave(image, time_steps=10, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Convert an image tensor [C, H, W] into a spike-wave tensor [time_steps, C, H, W].\n",
    "    A simple threshold-based latency coding is used.\n",
    "    \"\"\"\n",
    "    # Normalize image to [0, 1] if necessary\n",
    "    image = image / image.max()\n",
    "    # Determine spike time: lower intensity = later spike\n",
    "    # Here we simply invert the image intensity and scale to time steps.\n",
    "    spike_times = (1 - image) * (time_steps - 1)\n",
    "    spike_times = spike_times.long()\n",
    "    \n",
    "    # Create a binary spike-wave tensor\n",
    "    C, H, W = image.shape\n",
    "    spike_wave = torch.zeros(time_steps, C, H, W)\n",
    "    for t in range(time_steps):\n",
    "        spike_wave[t] = (spike_times <= t).float()  # Spike becomes 1 from its spike time onward\n",
    "    return spike_wave\n",
    "\n",
    "# Test the transformation on a sample image from MNIST\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "mnist_sample = transform(torchvision.datasets.MNIST(root='./data', train=True, download=True)[0][0])\n",
    "spike_wave_sample = image_to_spike_wave(mnist_sample, time_steps=10)\n",
    "print(\"Spike-wave shape:\", spike_wave_sample.shape)\n",
    "plt.imshow(spike_wave_sample[-1][0].numpy(), cmap='gray')\n",
    "plt.title(\"Spike-wave at final time step\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5. Define the Overall Network Architecture\n",
    "This cell defines a simple network with two spiking convolutional layers and a fully connected layer for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 39\u001b[0m\n\u001b[1;32m     37\u001b[0m model \u001b[38;5;241m=\u001b[39m SpikingCNN(neuron_params\u001b[38;5;241m=\u001b[39mneuron_params, time_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     38\u001b[0m sample_image \u001b[38;5;241m=\u001b[39m mnist_sample\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)  \u001b[38;5;66;03m# add batch dimension\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_image\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNetwork output shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, output\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.18/lib/python3.9/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.18/lib/python3.9/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[7], line 17\u001b[0m, in \u001b[0;36mSpikingCNN.forward\u001b[0;34m(self, input_image)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_image):\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# Convert image to spike-wave representation\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m     spike_wave \u001b[38;5;241m=\u001b[39m \u001b[43mimage_to_spike_wave\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtime_steps\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# shape: [T, C, H, W]\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;66;03m# Add batch dimension if needed: assume input_image shape [1, C, H, W]\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     spike_wave \u001b[38;5;241m=\u001b[39m spike_wave\u001b[38;5;241m.\u001b[39mto(device)\n",
      "Cell \u001b[0;32mIn[6], line 15\u001b[0m, in \u001b[0;36mimage_to_spike_wave\u001b[0;34m(image, time_steps, threshold)\u001b[0m\n\u001b[1;32m     12\u001b[0m spike_times \u001b[38;5;241m=\u001b[39m spike_times\u001b[38;5;241m.\u001b[39mlong()\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Create a binary spike-wave tensor\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m C, H, W \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m     16\u001b[0m spike_wave \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(time_steps, C, H, W)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(time_steps):\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 3)"
     ]
    }
   ],
   "source": [
    "# Step 5: Define the Overall Spiking CNN Architecture\n",
    "class SpikingCNN(nn.Module):\n",
    "    def __init__(self, neuron_params, time_steps=10):\n",
    "        super(SpikingCNN, self).__init__()\n",
    "        self.time_steps = time_steps\n",
    "        \n",
    "        # Define two spiking convolutional layers\n",
    "        self.layer1 = SpikingConvLayer(in_channels=1, out_channels=20, kernel_size=5, neuron_params=neuron_params, padding=2)\n",
    "        self.layer2 = SpikingConvLayer(in_channels=20, out_channels=50, kernel_size=5, neuron_params=neuron_params, padding=2)\n",
    "        \n",
    "        # Fully connected layer for classification\n",
    "        # Here, we assume the final spatial dimension remains 28x28; adjust pooling as needed.\n",
    "        self.fc = nn.Linear(50 * 28 * 28, 10)\n",
    "    \n",
    "    def forward(self, input_image):\n",
    "        # Convert image to spike-wave representation\n",
    "        spike_wave = image_to_spike_wave(input_image, time_steps=self.time_steps)  # shape: [T, C, H, W]\n",
    "        \n",
    "        # Add batch dimension if needed: assume input_image shape [1, C, H, W]\n",
    "        spike_wave = spike_wave.to(device)\n",
    "        \n",
    "        # Process through first spiking conv layer (apply on the last time slice or accumulate over time)\n",
    "        spikes1, v1, u1 = self.layer1(spike_wave, self.time_steps)\n",
    "        # For simplicity, take the last time-step output (or use pooling over time)\n",
    "        spikes1_last = spikes1[-1]\n",
    "        \n",
    "        spikes2, v2, u2 = self.layer2(spikes1_last, self.time_steps)\n",
    "        spikes2_last = spikes2[-1]\n",
    "        \n",
    "        # Flatten and feed into fully connected layer\n",
    "        out_flat = spikes2_last.view(spikes2_last.size(0), -1)\n",
    "        out = self.fc(out_flat)\n",
    "        return out\n",
    "\n",
    "# Instantiate the network and test on a single image\n",
    "neuron_params = (0.02, 0.2, -65, 8)\n",
    "model = SpikingCNN(neuron_params=neuron_params, time_steps=10).to(device)\n",
    "sample_image = mnist_sample.unsqueeze(0).to(device)  # add batch dimension\n",
    "output = model(sample_image)\n",
    "print(\"Network output shape:\", output.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 6. Define Training Functions Including STDP/R-STDP Placeholders\n",
    "In this cell we create a training loop. The code includes a standard loss–backpropagation branch for demonstration and placeholders for incorporating STDP or reward-modulated STDP. (In a complete spiking network, you might replace backprop with your plasticity rules.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 51\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# Train for a few epochs\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m4\u001b[39m):\n\u001b[0;32m---> 51\u001b[0m     avg_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAverage Loss for epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[8], line 15\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, data_loader, optimizer, criterion, epoch)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m images:\n\u001b[1;32m     14\u001b[0m     img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# add batch dimension\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     outputs\u001b[38;5;241m.\u001b[39mappend(output)\n\u001b[1;32m     17\u001b[0m outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(outputs, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.18/lib/python3.9/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.18/lib/python3.9/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[7], line 17\u001b[0m, in \u001b[0;36mSpikingCNN.forward\u001b[0;34m(self, input_image)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_image):\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# Convert image to spike-wave representation\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m     spike_wave \u001b[38;5;241m=\u001b[39m \u001b[43mimage_to_spike_wave\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtime_steps\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# shape: [T, C, H, W]\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;66;03m# Add batch dimension if needed: assume input_image shape [1, C, H, W]\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     spike_wave \u001b[38;5;241m=\u001b[39m spike_wave\u001b[38;5;241m.\u001b[39mto(device)\n",
      "Cell \u001b[0;32mIn[6], line 15\u001b[0m, in \u001b[0;36mimage_to_spike_wave\u001b[0;34m(image, time_steps, threshold)\u001b[0m\n\u001b[1;32m     12\u001b[0m spike_times \u001b[38;5;241m=\u001b[39m spike_times\u001b[38;5;241m.\u001b[39mlong()\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Create a binary spike-wave tensor\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m C, H, W \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m     16\u001b[0m spike_wave \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(time_steps, C, H, W)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(time_steps):\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 3)"
     ]
    }
   ],
   "source": [
    "# Step 6: Training Loop and STDP/R-STDP Placeholders\n",
    "def train_epoch(model, data_loader, optimizer, criterion, epoch):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch_idx, (images, labels) in enumerate(data_loader):\n",
    "        # Move data to device\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        # For simplicity, process one image at a time\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Process each image individually (or modify for batch processing)\n",
    "        outputs = []\n",
    "        for img in images:\n",
    "            img = img.unsqueeze(0)  # add batch dimension\n",
    "            output = model(img)\n",
    "            outputs.append(output)\n",
    "        outputs = torch.cat(outputs, dim=0)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # ----- STDP / R-STDP Placeholder -----\n",
    "        # Here you would add your custom plasticity rule updates (STDP / R-STDP)\n",
    "        # For example:\n",
    "        # if correct_prediction:\n",
    "        #     apply_reward_modulation()\n",
    "        # else:\n",
    "        #     apply_punishment_modulation()\n",
    "        # --------------------------------------\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f\"Epoch [{epoch}], Batch [{batch_idx}], Loss: {loss.item():.4f}\")\n",
    "    return running_loss / len(data_loader)\n",
    "\n",
    "# Setup MNIST dataset and DataLoader\n",
    "transform_mnist = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform_mnist)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Train for a few epochs\n",
    "for epoch in range(1, 4):\n",
    "    avg_loss = train_epoch(model, train_loader, optimizer, criterion, epoch)\n",
    "    print(f\"Average Loss for epoch {epoch}: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 7. Evaluation\n",
    "Finally, add a cell to evaluate your network on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m test_loader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(test_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 12\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(model, data_loader)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m images:\n\u001b[1;32m     11\u001b[0m     img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     outputs\u001b[38;5;241m.\u001b[39mappend(output)\n\u001b[1;32m     14\u001b[0m outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(outputs, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.18/lib/python3.9/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.18/lib/python3.9/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[7], line 17\u001b[0m, in \u001b[0;36mSpikingCNN.forward\u001b[0;34m(self, input_image)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_image):\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# Convert image to spike-wave representation\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m     spike_wave \u001b[38;5;241m=\u001b[39m \u001b[43mimage_to_spike_wave\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtime_steps\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# shape: [T, C, H, W]\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;66;03m# Add batch dimension if needed: assume input_image shape [1, C, H, W]\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     spike_wave \u001b[38;5;241m=\u001b[39m spike_wave\u001b[38;5;241m.\u001b[39mto(device)\n",
      "Cell \u001b[0;32mIn[6], line 15\u001b[0m, in \u001b[0;36mimage_to_spike_wave\u001b[0;34m(image, time_steps, threshold)\u001b[0m\n\u001b[1;32m     12\u001b[0m spike_times \u001b[38;5;241m=\u001b[39m spike_times\u001b[38;5;241m.\u001b[39mlong()\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Create a binary spike-wave tensor\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m C, H, W \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m     16\u001b[0m spike_wave \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(time_steps, C, H, W)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(time_steps):\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 3)"
     ]
    }
   ],
   "source": [
    "# Step 7: Evaluation Function\n",
    "def evaluate(model, data_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = []\n",
    "            for img in images:\n",
    "                img = img.unsqueeze(0)\n",
    "                output = model(img)\n",
    "                outputs.append(output)\n",
    "            outputs = torch.cat(outputs, dim=0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print(f\"Accuracy: {100 * correct / total:.2f}%\")\n",
    "\n",
    "# Setup MNIST test dataset and DataLoader\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform_mnist)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate(model, test_loader)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
